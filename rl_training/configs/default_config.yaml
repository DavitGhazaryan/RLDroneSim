# =============================================================================
# PID RL Training Configuration File
# =============================================================================
# This file contains all configuration parameters for training DDPG agents
# on the ArduPilot environment. Modify the values below to experiment with
# different hyperparameters without changing the code.
# =============================================================================

environment_config:
  max_episode_steps: 100
  mode: "altitude"               # Flight mode: position, attitude, stabilize, altitude
  observable_gains: "PSC_POSZ_P+PSC_VELZ_P+PSC_VELZ_I+PSC_VELZ_D+PSC_ACCZ_P+PSC_ACCZ_I+PSC_ACCZ_D"  # Which PID gains to observe
  observable_states: "alt_err+vZ_err+accZ_err" # Which states to observe
  action_gains: "PSC_POSZ_P+PSC_VELZ_P+PSC_VELZ_I+PSC_VELZ_D+PSC_ACCZ_P+PSC_ACCZ_I+PSC_ACCZ_D"      # Which PID gains to adjust
  action_dt: 0.05                 # Time interval between action and observation
  takeoff_altitude: 5.0          #  meters
  verbose: false                  # controls custom logging not internal

training_config:
  total_timesteps: 1000000                        # Total training timesteps
  runs_base: "/home/pid_rl/rl_training/runs"            # Base directory for all runs
  algo: "ddpg"    # "ddpg", "td3"
  mission: "hover"                                
  model_name_prefix: "ddpg_ardupilot"      # "ddpg_ardupilot", "td3_ardupilot"        
  progress_bar: true                       
  reset_num_timesteps: false               


ddpg_params:
  learning_rate: 0.001                          # Learning rate for actor and critic networks
  buffer_size: 100000                           # Size of replay buffer
  learning_starts: 5000                         # Steps before learning begins
  batch_size: 1024                              # Batch size for training
  tau: 0.005                                    # Soft update coefficient for target networks
  gamma: 0.99                                   # Discount factor for future rewards
  train_freq: 1                                 # Training frequency (every N steps)
  gradient_steps: 3                             # Gradient steps per training update
  action_noise:                                 # Action noise for exploration
    type: "NormalActionNoise"                   # Noise type: NormalActionNoise
    mean: 0.0                                   # Mean of noise distribution
    sigma: 0.1                                  # Standard deviation of noise
  optimize_memory_usage: false                  # Whether to optimize memory usage
  n_steps: -1                                   # Steps per update (default -1)
  verbose: 1                                    # Verbosity level
  policy_kwargs:
    net_arch: 
      pi: [256, 256]  # Actor (policy) network architecture
      qf: [256, 256]  # Critic (value) network architecture
  seed: null                                     # Random seed (optional)
  device: "cuda"                                # Device for training (auto, cpu, cuda)
  _init_setup_model: true                       # For DDPG specific initialization
  tensorboard_log: "/path/to/tensorboard/log"   # Path for TensorBoard logging


td3_params:
  learning_rate: 0.001                          # Learning rate for actor and critic networks
  buffer_size: 1000000                          # Size of replay buffer
  learning_starts: 5000                         # Steps before learning begins
  batch_size: 1024                              # Batch size for training
  tau: 0.005                                    # Soft update coefficient for target networks
  gamma: 0.99                                   # Discount factor for future rewards
  train_freq: 1                                 # Training frequency (every N steps)
  gradient_steps: 15                            # Gradient steps per training update
  action_noise:                                 # Action noise for exploration
    type: "NormalActionNoise"                   # Noise type: NormalActionNoise
    mean: 0.0                                   # Mean of noise distribution
    sigma: 0.05                                  # Standard deviation of noise
  verbose: 1                                    # Verbosity level
  policy_kwargs:
    net_arch: 
      pi: [256, 256]  # Actor (policy) network architecture
      qf: [256, 256]  # Critic (value) network architecture
  target_policy_noise: 0.2
  target_noise_clip: 0.5                        # Clip range for target noise
  policy_delay: 3                               # Number of updates before updating actor
  n_steps: -1                                   # Steps per update (default -1)
  stats_window_size: 100                        # Size of the statistics window for updates
  device: "cuda"                                # Device for training (auto, cpu, cuda)
  _init_setup_model: true                       # For TD3 specific initialization



callbacks:
  - type: "checkpoint"                         # Checkpoint callback type
    save_freq: 500                             # Save frequency for checkpoints
    name_prefix: "ddpg_ardupilot"      
  - type: "tensorboard"                         # Checkpoint callback type

reward_config:
  altitude:                        # Mode
    alt_w: 1                       # Weight for altitude error
    xy_w: 1                        # Weight for XY position error
    velN_w: 1                      # Weight for North velocity err
    velE_w: 1                      # Weight for East velocity err
    velZ_w: 1                      # Weight for Down velocity err
    accN_w: 1                      # Weight for North acceleration err
    accE_w: 1                      # Weight for East acceleration err
    accZ_w: 1                      # Weight for Down acceleration err
    acc_yaw_w: 1                   # Weight for yaw acceleration err
    stable_time_coef: 10
    success_reward: 2000
    crash_penalty_att:  -20                      # Penalty for crashes
    crash_penalty_vel:  -20                      # Penalty for crashes
    crash_penalty_flip: -20                     # Penalty for crashes
    crash_penalty_far:  -20                      # Penalty for crashes
    step_reward: 100                            # Penalty for crashes
    tolerance: 
      xy_tol: 3.0           # cm
      alt_tol: 0.03         #
      vel_n_tol: 2.0        #  ?
      vel_e_tol: 2.0        #  ?
      vel_d_tol: 3.0        #  ?
      acc_n_tol: 0.5        #  ? 
      acc_e_tol: 0.003      #  ? 
      acc_yaw_tol: 0.002    #  ? 
      acc_d_tol: 8.8        #  ? 


drone_config:
  ip: "udp:127.0.0.1"
  master_port: 5760                            # default udp port
  port_check_timeout: 5.0                     # Port availability timeout

sitl_config:
  ardupilot_path: "/home/pid_rl/ardupilot"     # Path to ArduPilot directory
  frame: "gazebo-iris"                         # Vehicle model for Gazebo
  name: "iris_with_ardupilot"                  # Vehicle name
  ideal_sensors: true                          # Use ideal sensor models
  count: null                                  # Number of vehicle instances
  location: null                               # Home location: [lat, lon, alt, yaw]
  speedup: 10                                   # Simulation speed multiplier
  wipe_eeprom: true                            # Clear EEPROM on startup
  use_dir: null                                # Custom directory for SITL
  delay_start: null                            # Startup delay in seconds
  model: "JSON"                                # Vehicle model format
  clean: true                                  # Clean build before starting
  no_rebuild: true                             # Skip rebuilding
  no_configure: true                           # Skip configuration
  no_mavproxy: true                           # Launch MAVProxy
  udp: true                                    # Use UDP communication
  map: false                                   # Show map interface
  console: false                               # Show console interface
  ip: "udp:127.0.0.1"
  master_port: 5760                            # default udp port
  timeout: 30.0                                # SITL startup timeout
  min_startup_delay: 5.0                       # Minimum startup delay
  port_check_timeout: 5.0                     # Port availability timeout

# Gazebo configuration
# control the wind from the sdf file directly
gazebo_config:
  sdf_file: '/home/pid_rl/ardupilot_gazebo/worlds/simple_world.sdf'  # always write the version for 1 instance 
  gui: false                                         
  verbose: false          # enables -v 4; internal logging of gz            
