# =============================================================================
# PID RL Training Configuration File
# =============================================================================
# This file contains all configuration parameters for training DDPG agents
# on the ArduPilot environment. Modify the values below to experiment with
# different hyperparameters without changing the code.
# =============================================================================

environment_config:
  max_episode_steps: 100
  mode: "altitude"               # Flight mode: position, attitude, stabilize, altitude
  observable_gains: "PSC_POSZ_P+PSC_VELZ_P+PSC_VELZ_I+PSC_VELZ_D+PSC_ACCZ_P+PSC_ACCZ_I+PSC_ACCZ_D"  # Which PID gains to observe
  observable_states: "alt_err+vZ_err+accZ_err" # Which states to observe
  action_gains: "PSC_POSZ_P+PSC_VELZ_P+PSC_VELZ_I+PSC_VELZ_D+PSC_ACCZ_P+PSC_ACCZ_I+PSC_ACCZ_D"      # Which PID gains to adjust
  reward_config: "hover"       
  action_dt: 1.0                 # Time interval between action and observation
  takeoff_altitude: 5.0          #  meters
  verbose: false                  # controls custom logging not internal
  # normalize_observations: true   # Whether to normalize observations
  # normalize_actions: false       # Whether to normalize actions

training_config:
  total_timesteps: 1000000                        # Total training timesteps
  log_freq: 50                                 # Logging frequency
  runs_base: "/home/pid_rl/rl_training/runs"            # Base directory for all runs
  algo: "ddpg"                                 # Algorithm name for run dir
  mission: "hover"                                
  model_name_prefix: "ddpg_ardupilot"          # Model name prefix
  progress_bar: true                           # Show training progress bar
  reset_num_timesteps: true                    # Reset timestep counter


# DDPG algorithm parameters
# Core hyperparameters for the DDPG algorithm
ddpg_params:
  learning_rate: 0.001                          # Learning rate for actor and critic networks
  buffer_size: 100000                          # Size of replay buffer
  learning_starts: 5000                          # Steps before learning begins
  batch_size: 1024                               # Batch size for training
  tau: 0.005                                   # Soft update coefficient for target networks
  gamma: 0.99                                  # Discount factor for future rewards
  train_freq: 1                                # Training frequency (every N steps)
  gradient_steps: 3                             # Gradient steps per training update
  action_noise:                                # Action noise for exploration
    type: "NormalActionNoise"                   # Noise type: NormalActionNoise
    mean: 0.0                                  # Mean of noise distribution
    sigma: 0.1                                 # Standard deviation of noise
  # target_policy_noise: 0.2                     # Target policy noise (for TD3-like behavior)
  # target_noise_clip: 0.5                       # Target noise clipping
  verbose: 1                                   # Verbosity level
  policy_kwargs:
    net_arch: 
      pi: [256, 256]  # 2 hidden layers, each with 256 units (for both actor and critic)
      qf: [256, 256]  # 2 hidden layers, each with 256 units (for both actor and critic)
    # activation_fn: torch.nn.ReLU  # Activation function for both actor and critic networks
  device: "cuda"                               # Device for training (auto, cpu, cuda)
  _init_setup_model: true                      # Initialize model setup


# Callbacks configuration
# Callbacks for logging, checkpointing, and evaluation during training
callbacks:
  - type: "checkpoint"                         # Checkpoint callback type
    save_freq: 200                             # Save frequency for checkpoints
    name_prefix: "ddpg_ardupilot"              # Checkpoint name prefix

# Evaluation configuration
evaluation_config:
  n_eval_episodes: 20                           # Number of evaluation episodes
  deterministic: true                           # Use deterministic actions
  save_results: true                            # Save evaluation results
  plot_results: false                            # Plot evaluation results

# Reward function configuration
# Parameters for different reward function types
# Comment out sections you don't want to use or set weights to 0
reward_config:
  hover:                                       # Hover reward function
    alt_w: 0.9                       # Weight for altitude error
    xy_w: 0.8                        # Weight for XY position error
    velN_w: 0.06                      # Weight for North velocity err
    velE_w: 0.06                      # Weight for East velocity err
    velZ_w: 0.06                      # Weight for Down velocity err
    accN_w: 0.05                      # Weight for North acceleration err
    accE_w: 0.05                      # Weight for East acceleration err
    accZ_w: 0.05                      # Weight for Down acceleration err
    acc_yaw_w: 0.5                   # Weight for yaw acceleration err
    vicinity: 2
    success_reward: 2000
    crash_penalty_att: 400                      # Penalty for crashes
    crash_penalty_vel: 400                      # Penalty for crashes
    crash_penalty_flip: 400                     # Penalty for crashes
    crash_penalty_far: 400                      # Penalty for crashes
    step_reward: 800                            # Penalty for crashes



ardupilot_config:
  ardupilot_path: "/home/pid_rl/ardupilot"     # Path to ArduPilot directory
  frame: "gazebo-iris"                         # Vehicle model for Gazebo
  name: "iris_with_ardupilot"                  # Vehicle name
  ideal_sensors: true                          # Use ideal sensor models
  count: null                                  # Number of vehicle instances
  location: null                               # Home location: [lat, lon, alt, yaw]
  speedup: 3                                   # Simulation speed multiplier
  wipe_eeprom: true                            # Clear EEPROM on startup
  use_dir: null                                # Custom directory for SITL
  delay_start: null                            # Startup delay in seconds
  model: "JSON"                                # Vehicle model format
  clean: true                                  # Clean build before starting
  no_rebuild: true                             # Skip rebuilding
  no_configure: true                           # Skip configuration
  no_mavproxy: false                           # Launch MAVProxy
  udp: true                                    # Use UDP communication
  map: false                                   # Show map interface
  console: false                               # Show console interface
  mavproxy_args: null                          # Additional MAVProxy arguments
  timeout: 30.0                                # SITL startup timeout
  min_startup_delay: 5.0                       # Minimum startup delay
  mavsdk_port: 14550   # +10 for I 1           # MAVSDK port    
  master_port: 14551   # +10 for I 1           # MAVLink master port
  port_check_timeout: 30.0                     # Port availability timeout

# Gazebo configuration
# control the wind from the sdf file directly
gazebo_config:
  sdf_file: '/home/pid_rl/ardupilot_gazebo/worlds/simple_world.sdf'  # always write the version for 1 instance
  gui: true                                         
  verbose: false          # enables -v 4; internal logging of gz            
